\begin{abstract}
	Summarize problem, method and result in 150 words.
\end{abstract}

\section{Introduction}
Personality is often described in terms of five \emph{traits}\footnote{Although other are listed on
\href{http://en.wikipedia.org/wiki/Trait\_theory\#List\_of\_personality\_traits}{the
relevant Wikipedia page}.}, presented in \autocite{mairesse2007perso} as follow:
\begin{itemize}
\item Extraversion vs. Introversion (sociable, assertive, playful vs. aloof, reserved, shy)
\item Emotional stability vs. Neuroticism (calm, unemotional vs. insecure, anxious)
\item Agreeableness vs. Disagreeable (friendly, cooperative vs. antagonistic, faultfinding)
\item Conscientiousness vs. Unconscientious (self-disciplined, organised vs. inefficient, careless)
\item Openness to experience (intellectual, insightful vs. shallow, unimaginative)
\end{itemize}

In this project, we will consider that each of these dimensions is binary (whereas one may argue that no individual can be perfectly extroverted or introverted) and we will try to classify people in each dimension based on their writing. More specifically, some student were asked to produce a so called \emph{stream of consciousness} essay, meaning that they wrote their current thoughts freely for twenty minutes. \Textcite{pennebaker1999corpus} collected 2468 such essays which account for about 1.6 millions words\footnote{It is surprisingly difficult to come with a precise figure because of the ambiguity of the definition of a word.}. Furthermore, this dataset is labelled because they assessed the personality of each author with a standard questionnaire. The key idea is that the way we express ourselves (for instance by writing) reflect our personality. In Bayesian terms, we would say that the text is an observed variable which is conditioned by a hidden one, the personality. Naturally, 
Describe the data, the problem and related one (sentiment analysis), the
previous works\cite{mairesse2007perso}, the method that will be used.

\section{Pre processing}
Before doing any Classification, we need to perform several operations to transform the raw data contained in the file \texttt{essays.csv} into a document-term matrix, which a more suitable representation.
\begin{itemize}
	\item First, we need to separate for each line the text itself from the five labels, which cause no difficulties but is mentioned here for the sake of completeness. Another \enquote{easy but annoying} issue was that some characters generate encoding error (which was solved not elegantly by removing them, since there was only a few of them).
	\item I then wondered what to do about number not written in full. I changed single digit into equivalent word (as shown in \vref{tab:num}) and replace all the other by a single unique token (\texttt{xnumx}).
	\begin{table}[hb]
		\centering
	\begin{tabular}{cccccccccccc}
		\toprule
		& zero & one & two & three & four & five & six & seven & eight & nine &
		\emph{total} \tabularnewline
		\midrule
		raw & \numprint{18} & \numprint{4816} & \numprint{1193} &
		\numprint{518} & \numprint{287} & \numprint{276} & \numprint{126} &
		\numprint{95} & \numprint{64} & \numprint{60} & \numprint{7453}
		\tabularnewline
		converted & \numprint{134} & \numprint{5090} & \numprint{1814} &
		\numprint{1069} & \numprint{737} & \numprint{748} & \numprint{375} &
		\numprint{297} & \numprint{309} & \numprint{262} & \numprint{10835}
		\tabularnewline
		\bottomrule
	\end{tabular}
	\caption{Counts of the ten words representing digit. The first line referred to the raw data, while in the second, single digit number have been converted to the corresponding word. Although it is probably irrelevant, it is amusing to note most people write numbers with digit and not letters, especially if the number is not 1 (and to some extent, 2 and 3).}
	\label{tab:num}
\end{table}
\item To reduce the sparsity of data, I decide to perform a stemming step, even though is was not such a severe problem because the text are all in American English, which is a rather analytical language. Because I used the python language, I first looked at vavious algorithm offered by the NLTK library\autocite{bird2009nltk}. \texttt{nltk.WordNet} look (very slow, only transform plural noun to singular form), Porter\autocite{porter1980algo} and Snowball\autocite{porter2001snowball} algorithm (highly subjective but too aggressive was -> wa). So I choose hunspell\footnote{\href{http://hunspell.sourceforge.net/}{http://hunspell.sourceforge.net/}}, which has also issues (discard punctuation and different segmentation mid-day -> mid, day) and some bad alternative (thing -> thing or the+ING). For the latter, POS could have helped but because of the other problem, the two version of the text were no more aligned. But still good because it change moved to move for instance. Still fail for woke to wake
\item textual marker (case, punctuation, length)
\item POS tagging using \autocite{bird2009nltk} (I also consider
	using a CRF implementation \autocite[for instance][]{CRFsuite} but it
	require too much training for my goal).
\item Entity recognition
\item word counting
\end{itemize}

\section{Classification}
Dimensionality reduction: SVD, LDA, ICA

Method: MN NB, RBF C-SVM (maybe with text kernel?), cosine kNN

Result tables

\section{Conclusion}
